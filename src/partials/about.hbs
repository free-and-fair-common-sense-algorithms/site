<div class="about-section">
  <h3>Our mission is to develop and promote equitable, auditable, and accessible machine learning solutions that improve the quality of life for all people equally.</h3>
  <h3 class="mt-5 pt-2 thin-border-top">About</h3>
  <p>A new era of human history is being ushered with the rise of artificial intelligence and big data. Companies increasingly use machine learning to create automated systems to better connect people with content and advertisements, increase efficiency in manufacturing supply chains, and extract meaningful information from massive volumes of documents and images, to name a few. Algorithms promise to scale complex decision making beyond the limitations of humans: they increase efficiency (because they never become tired), they scale exponentially (they don’t require boundless people), and they are free of subjective human biases (they are based upon objective math).
  </p>
  <p>As the widespread adoption of these automated systems proliferates, governmental and public service organizations seek to benefit from these advances as well. Typically, private companies or contractors are paid exorbitantly to develop an algorithmic decision making system that replaces or streamlines an aspect of the organization’s work. These systems are built on historical data created during the human-based decision making process. This data is then fed into advanced mathematical programs called “machine learning models” that statistically identify and exploit patterns in the data. The result is a system that recreates the actions taken by people–the “deciders.” Algorithms like Equivant’s COMPAS, a pretrial risk assessment algorithm used in the criminal justice system, are designed to predict the likelihood of a defendant to commit a crime in the future, attempting to make it easier for judges to determine who should be incarcerated. Similarly, the healthcare industry employs algorithms to identify and assist patients with complex health needs, attempting to prioritize and provide care to those who are at a higher risk. However, when put to use, these algorithms rarely provide clear improvements over the status quo, and often these machine learning systems amplify latent biases in the data, creating or perpetuating greater harm to those on the receiving end of these algorithmic decisions–the “impacted”. The result is <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" target="_blank"><u>more low-risk people are sent into detention when they should have been freed</u></a>, and <a href="https://www.ehidc.org/sites/default/files/resources/files/Dissecting%20racial%20bias%20in%20an%20algorithm%20used%20to%20manage%20the%20health%20of%20populations.pdf" target="_blank"><u>unhealthy individuals are denied ample care while healthier people receive more attentive care</u></a>. These are a couple examples out of countless where algorithmic decision making systems exacerbate biases under the guise of an objective and mathematically sound solution.
  </p>
  <p>Automated decision making does not have to be this way. Algorithms can be designed to work in accordance with the interests of the people, the “impacted,” instead of in the interest of the organizations, the “deciders.” Free and Fair Commonsense Algorithms for Society (FAFCSAFS) was established to do just that: provide algorithmic decision making solutions that actually improve society. Imagine a world where the “deciders” aren’t burdened by making complex and tough choices, and the “impacted” know that decisions are made with their best interest in mind; this is FAFCSAFS.
  </p>
</div>
